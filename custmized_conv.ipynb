{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on the conv\n",
    "* kernel size of time should be 1, otherwise it gets convolved as well.\n",
    "* bias=False\n",
    "* replace their pool layer with nn.AvgPool3d. The pool they used is a $sum*\\theta$ not an average.\n",
    "* should recieve the Spikes from the previous layer\n",
    "\n",
    "## Pipeline\n",
    "raw data(B,C,H,W) --- spike generation ---> spike(B,C,H,W,T) --- Conv3d on spikes ---> X(t) for every t of T --- eq 1: LIF, threshold and reset ---> intermediate V(t), the membrane potential --- based on V(t) ---> Spikes for the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = torch.abs(torch.rand(2,1,4,4,6)) # (batch_size, channels, H, W, Time)\n",
    "# conv3d = nn.Conv3d(1,2,kernel_size=(4,4,1),bias=False)\n",
    "# conv_spikes = conv3d(sample_batch)\n",
    "# conv_spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 3 classes we should be able to use them directly\n",
    "## We need spikes from spike generatior as input and the spike counter, then they should work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LIF(spikes, theta, leak, V_min):\n",
    "    \n",
    "        '''\n",
    "        Integrate-and-fire: given a tensor with shape (B,C,H,W,T), loop over T\n",
    "\n",
    "        Params: \n",
    "            spikes: the spikes from previous layer, containing 0 or 1.\n",
    "            theta: threshold to fire a spike.\n",
    "            l: leakage parameter.\n",
    "            V_min: the resting state of membrane potential, usually is set to 0.\n",
    "\n",
    "        return:\n",
    "            next_spikes: 0 or 1 tensor with the same shape of input\n",
    "\n",
    "        '''\n",
    "        \n",
    "        # the padding controls where to pad, first two of the tuple control the last dim of the tensor\n",
    "        _pad = nn.ConstantPad3d((1,0,0,0,0,0), 0)\n",
    "        pad_spikes = _pad(spikes)\n",
    "\n",
    "        V = torch.zeros_like(pad_spikes)\n",
    "        next_Spikes = torch.zeros_like(pad_spikes)\n",
    "\n",
    "        T = pad_spikes.shape[-1]\n",
    "\n",
    "        for t in range(1, T):\n",
    "            # equation (1a)\n",
    "            V[:,:,:,:,t] = V[:,:,:,:,t-1] + leak + pad_spikes[:,:,:,:,t]\n",
    "            # thresholding and fire spike (1b)\n",
    "            mask_threshold = V[:,:,:,:,t] >= theta\n",
    "            next_Spikes[:,:,:,:,t][mask_threshold] = 1        \n",
    "            # reset the potential to zero \n",
    "            V[:,:,:,:,t][mask_threshold] = 0\n",
    "            \n",
    "            # reset the value to V_min if drops below (1c)\n",
    "            mask_min = (V[:,:,:,:,t] < V_min)\n",
    "            V[:,:,:,:,t][mask_min] = V_min\n",
    "\n",
    "        return (V[:,:,:,:,1:], next_Spikes[:,:,:,:,1:])\n",
    "\n",
    "\n",
    "\n",
    "class convLayer(nn.Conv3d):\n",
    "\n",
    "    def __init__(self, inChannels, outChannels, kernelSize, theta, leak=0, V_min=0):\n",
    "        \n",
    "        kernel = (kernelSize, kernelSize, 1)\n",
    "        \n",
    "        super(convLayer, self).__init__(inChannels, outChannels, kernel, bias=False)\n",
    "        self.theta = theta\n",
    "        self.leak = leak\n",
    "        self.V_min = V_min\n",
    "\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        # get X, namely eq(2)\n",
    "        conv_spikes = F.conv3d(input, \n",
    "                        self.weight, self.bias, \n",
    "                        self.stride, self.padding, self.dilation, self.groups)\n",
    "        \n",
    "        output = LIF(conv_spikes, self.theta, self.leak, self.V_min)\n",
    "        \n",
    "        return (conv_spikes, *output)\n",
    "\n",
    "    \n",
    "class poolLayer(nn.AvgPool3d):\n",
    "    def __init__(self, kernel_size, theta, leak=0, V_min=0):\n",
    "        super(poolLayer, self).__init__(kernel_size)\n",
    "        self.theta = theta\n",
    "        self.leak = leak\n",
    "        self.V_min = V_min\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        # get X, namely eq(2)\n",
    "        pool_spikes = F.avg_pool3d(input, self.kernel_size)\n",
    "        \n",
    "        output = LIF(pool_spikes, self.theta, self.leak, self.V_min)\n",
    "        \n",
    "        return (pool_spikes, *output)\n",
    "\n",
    "class denseLayer(nn.Conv3d):\n",
    "    def __init__(self, inFeatures, outFeatures, theta, leak=0, V_min=0):\n",
    "        '''\n",
    "        '''\n",
    "        # extract information for kernel and inChannels\n",
    "        if type(inFeatures) == int:\n",
    "            kernel = (1, 1, 1)\n",
    "            inChannels = inFeatures \n",
    "        elif len(inFeatures) == 2:\n",
    "            kernel = (inFeatures[1], inFeatures[0], 1)\n",
    "            inChannels = 1\n",
    "        elif len(inFeatures) == 3:\n",
    "            kernel = (inFeatures[1], inFeatures[0], 1)\n",
    "            inChannels = inFeatures[2]\n",
    "        else:\n",
    "            raise Exception('inFeatures should not be more than 3 dimension. It was: {}'.format(inFeatures.shape))\n",
    "\n",
    "        \n",
    "        if type(outFeatures) == int:\n",
    "            outChannels = outFeatures\n",
    "        else:\n",
    "            raise Exception('outFeatures should not be more than 1 dimesnion. It was: {}'.format(outFeatures.shape))\n",
    "\n",
    "        \n",
    "        super(denseLayer, self).__init__(inChannels, outChannels, kernel, bias=False)\n",
    "        \n",
    "        # params for the LIF\n",
    "        self.theta = theta\n",
    "        self.leak = leak\n",
    "        self.V_min = V_min\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        fc_spikes = F.conv3d(input, \n",
    "                        self.weight, self.bias, \n",
    "                        self.stride, self.padding, self.dilation, self.groups)\n",
    "        \n",
    "        output = LIF(fc_spikes, self.theta, self.leak, self.V_min)\n",
    "        \n",
    "        return (fc_spikes, *output)\n",
    "\n",
    "slayer_conv = convLayer(1,2,4,theta = 0.5)\n",
    "slayer_pool = poolLayer((2,2,1),theta=0.5)\n",
    "slayer_dense = denseLayer((1,1,2), 10, theta = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 1, 1, 6])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slayer_conv(sample_batch)[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 1, 1, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slayer_dense(slayer_conv(sample_batch)[2])[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2, 2, 6])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slayer_pool(sample_batch)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4, 4, 6])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.3815, 0.4638, 0.7046, 0.7369, 0.6491, 0.4967],\n",
       "           [0.6913, 0.5906, 0.6854, 0.2956, 0.3159, 0.2807]],\n",
       "\n",
       "          [[0.5666, 0.3653, 0.6465, 0.5139, 0.3165, 0.4897],\n",
       "           [0.5576, 0.6426, 0.5479, 0.5369, 0.6438, 0.4931]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.5946, 0.5295, 0.4178, 0.6139, 0.3208, 0.5947],\n",
       "           [0.3775, 0.6151, 0.5779, 0.6122, 0.7627, 0.4354]],\n",
       "\n",
       "          [[0.3299, 0.5560, 0.3300, 0.5514, 0.7094, 0.5810],\n",
       "           [0.5793, 0.3344, 0.7445, 0.6099, 0.7501, 0.4990]]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.AvgPool3d((2,2,1))\n",
    "pool(sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2, 2, 6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool(sample_batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# theta = 0.5\n",
    "# V_min = -0.1\n",
    "# Leak = 0\n",
    "\n",
    "# conv_spikes = conv3d(sample_batch)\n",
    "# print(sample_batch.shape)\n",
    "# print(conv_spikes.shape)\n",
    "\n",
    "# # the padding controls where to pad, first two of the tuple control the last dim of the tensor\n",
    "# _pad = nn.ConstantPad3d((1,0,0,0,0,0), 0)\n",
    "# conv_pad_spikes = _pad(conv_spikes)\n",
    "\n",
    "# V = torch.zeros_like(conv_pad_spikes)\n",
    "# next_Spikes = torch.zeros_like(conv_pad_spikes)\n",
    "\n",
    "# T = conv_pad_spikes.shape[-1]\n",
    "\n",
    "\n",
    "# for t in range(1, T):\n",
    "#     print('_______calculate membrane_________')\n",
    "#     # equation 1a\n",
    "#     V[:,:,:,:,t] = V[:,:,:,:,t-1] + Leak + conv_pad_spikes[:,:,:,:,t]\n",
    "#     print(V)\n",
    "#     # thresholding and fire spike\n",
    "#     mask_threshold = V[:,:,:,:,t] >= theta\n",
    "#     next_Spikes[:,:,:,:,t][mask_threshold] = 1        \n",
    "#     # reset the potential to zero \n",
    "#     V[:,:,:,:,t][mask_threshold] = 0\n",
    "    \n",
    "# #     mask_min = (V[:,:,:,:,t]<V_min)\n",
    "# #     V[:,:,:,:,t][mask_min] = V_min\n",
    "#     print('_____Spike!________')\n",
    "#     print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     4
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "theta = 0.5\n",
    "V_min = -0.1\n",
    "Leak = 0\n",
    "\n",
    "def LIF(spikes, theta=0.5, Leak=0, V_min=0):\n",
    "    \n",
    "    '''\n",
    "    Integrate-and-fire: given a tensor with shape (B,C,H,W,T), loop over \n",
    "    \n",
    "    Params:\n",
    "        spikes: the spikes from previous layer, containing 0 or 1.\n",
    "        theta: threshold to fire a spike.\n",
    "        L: leakage parameter.\n",
    "        V_min: the resting state of membrane potential, usually is set to 0.\n",
    "        \n",
    "    return:\n",
    "        next_spikes: 0 or 1 tensor with the same shape of input\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # the padding controls where to pad, first two of the tuple control the last dim of the tensor\n",
    "    _pad = nn.ConstantPad3d((1,0,0,0,0,0), 0)\n",
    "    pad_spikes = _pad(spikes)\n",
    "    \n",
    "    V = torch.zeros_like(pad_spikes)\n",
    "    next_Spikes = torch.zeros_like(pad_spikes)\n",
    "    \n",
    "    T = pad_spikes.shape[-1]\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        # equation 1a\n",
    "        V[:,:,:,:,t] = V[:,:,:,:,t-1] + Leak + pad_spikes[:,:,:,:,t]\n",
    "        # thresholding and fire spike\n",
    "        mask_threshold = V[:,:,:,:,t] >= theta\n",
    "        next_Spikes[:,:,:,:,t][mask_threshold] = 1        \n",
    "        # reset the potential to zero \n",
    "        V[:,:,:,:,t][mask_threshold] = 0\n",
    "\n",
    "        mask_min = (V[:,:,:,:,t]<V_min)\n",
    "        V[:,:,:,:,t][mask_min] = V_min\n",
    "        \n",
    "        \n",
    "    return V[:,:,:,:,1:], next_Spikes[:,:,:,:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 0.6851,  0.4956,  0.4907,  0.5220,  0.5770,  0.1868]]],\n",
       "\n",
       "\n",
       "         [[[-0.4270, -0.3262, -0.2336, -0.4638, -0.1081, -0.4898]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 0.9438,  0.5806,  0.6456,  0.5263,  0.4105,  0.7090]]],\n",
       "\n",
       "\n",
       "         [[[-0.0026, -0.1107, -0.3080, -0.4042, -0.2310, -0.7148]]]]],\n",
       "       grad_fn=<ThnnConv3DBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = torch.abs(torch.rand(2,1,4,4,6)) # (batch_size, channels, H, W, Time)\n",
    "conv3d = nn.Conv3d(1,2,kernel_size=(4,4,1),bias=False)\n",
    "conv_spikes = conv3d(sample_batch)\n",
    "conv_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_spikes = torch.ones(2,1,4,4,15) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[[0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "           [[0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "           [[0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "           [[0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "           [[0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "           [[0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000]],\n",
       " \n",
       "           [[0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000],\n",
       "            [0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000, 0.4000,\n",
       "             0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.2000, 0.3000]]]]]),\n",
       " tensor([[[[[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.]]]],\n",
       " \n",
       " \n",
       " \n",
       "         [[[[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "            [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.]]]]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIF(check_spikes,theta, V_min=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.0000, 0.4956, 0.0000, 0.0000, 0.0000, 0.1868]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.0000, 0.0000, 0.0000, 0.0000, 0.4105, 0.0000]]],\n",
       "\n",
       "\n",
       "         [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIF(conv_spikes,theta)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 0.6851,  0.4956,  0.4907,  0.5220,  0.5770,  0.1868]]],\n",
       "\n",
       "\n",
       "         [[[-0.4270, -0.3262, -0.2336, -0.4638, -0.1081, -0.4898]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 0.9438,  0.5806,  0.6456,  0.5263,  0.4105,  0.7090]]],\n",
       "\n",
       "\n",
       "         [[[-0.0026, -0.1107, -0.3080, -0.4042, -0.2310, -0.7148]]]]],\n",
       "       grad_fn=<ThnnConv3DBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.8275, 1.4816, 0.9640, 0.4944, 1.1763, 1.4621],\n",
       "           [0.0550, 0.6530, 0.8653, 0.6185, 0.7270, 0.5225],\n",
       "           [0.4607, 0.6692, 1.0878, 1.1623, 0.9898, 1.2918],\n",
       "           [0.7881, 0.8658, 0.9896, 1.6471, 0.8356, 1.0450]],\n",
       "\n",
       "          [[0.6149, 1.1778, 1.3756, 1.1751, 0.6608, 1.1548],\n",
       "           [0.0797, 0.4623, 0.4922, 0.3192, 1.1126, 1.3341],\n",
       "           [0.7701, 1.5521, 1.5145, 1.5825, 1.3025, 0.9577],\n",
       "           [0.4127, 0.5290, 0.5073, 0.9474, 0.6939, 1.0737]],\n",
       "\n",
       "          [[0.9167, 1.6864, 1.5628, 0.9006, 0.9320, 0.9134],\n",
       "           [0.9075, 1.2440, 0.6663, 0.4959, 0.6972, 1.4349],\n",
       "           [0.9342, 1.4226, 0.5464, 0.5636, 1.2331, 1.4368],\n",
       "           [0.2386, 0.5454, 1.0050, 1.2593, 1.2872, 1.7125]],\n",
       "\n",
       "          [[0.8743, 1.7199, 1.0557, 0.7375, 0.9895, 0.5349],\n",
       "           [0.8573, 1.1217, 1.0049, 1.3333, 1.0664, 0.5724],\n",
       "           [0.0146, 0.2119, 1.0529, 1.5894, 1.6911, 1.5275],\n",
       "           [0.2773, 1.1240, 1.7146, 1.8235, 1.5088, 0.8226]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.3365, 0.8683, 1.0704, 1.4709, 1.6713, 0.9928],\n",
       "           [0.2517, 0.9102, 1.5482, 0.9256, 0.4394, 0.9630],\n",
       "           [0.6116, 1.3190, 0.8999, 0.3635, 0.2721, 1.0536],\n",
       "           [0.5754, 0.6032, 0.5698, 1.3450, 1.3417, 0.6471]],\n",
       "\n",
       "          [[0.9828, 1.8488, 1.0438, 0.4246, 0.3432, 0.8300],\n",
       "           [0.0552, 0.6688, 0.7721, 1.0825, 1.5204, 0.9242],\n",
       "           [0.8588, 1.5471, 1.5473, 1.4066, 1.4865, 1.5395],\n",
       "           [0.4414, 0.9638, 0.9719, 1.0378, 1.0988, 0.8697]],\n",
       "\n",
       "          [[0.5839, 0.6273, 0.6030, 1.3083, 0.8038, 0.3594],\n",
       "           [0.9048, 0.9886, 0.5649, 0.6260, 0.8979, 1.2130],\n",
       "           [0.5947, 1.4837, 0.9271, 0.9327, 1.3244, 1.4010],\n",
       "           [0.4667, 0.9712, 0.9519, 0.6627, 0.5269, 1.2261]],\n",
       "\n",
       "          [[0.4996, 1.1156, 0.9837, 1.1410, 0.8830, 1.0470],\n",
       "           [0.2978, 0.5608, 0.8757, 1.1549, 1.3667, 1.7848],\n",
       "           [0.3302, 0.8441, 0.9467, 0.7138, 0.9887, 1.5656],\n",
       "           [0.2837, 0.8718, 0.6833, 0.8170, 1.2428, 0.6546]]]]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv3d(_pad(sample_batch),weight=torch.ones(1,1,1,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.flip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
