{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "\n",
    "from custmized_layer import convLayer, poolLayer, denseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Inline')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in Cao's paper they used uniform encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b2b21b5e1287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     def __init__(self, datasetPath, samplingTime, sampleLength, small=True, train=True, encoding='uniform',\n\u001b[1;32m     26\u001b[0m                  mode='classification'):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# func to generate spikes\n",
    "\n",
    "def poisson_spike(x, time_bins):\n",
    "    shape_org = list(x.shape)\n",
    "    y = x.reshape(-1)\n",
    "    samples = []\n",
    "    for yy in y:\n",
    "        m1 = Poisson(yy)\n",
    "        samples.append(m1.sample(sample_shape=(time_bins,)) > 0)\n",
    "    output = torch.stack(samples, dim=0).float()\n",
    "    return output.reshape(shape_org + [time_bins])\n",
    "\n",
    "\n",
    "def uniform_spike(x, time_bins):\n",
    "    shape_org = list(x.shape)\n",
    "    shape_target = shape_org + [time_bins]\n",
    "    output = torch.rand(shape_target)\n",
    "    a = x.unsqueeze(-1)\n",
    "    b = torch.cat(time_bins * [a], dim=-1)\n",
    "    C = 0.33\n",
    "    output = (C * b > output)\n",
    "    return output.float()\n",
    "\n",
    "class SMNIST(Dataset):\n",
    "    def __init__(self, datasetPath, samplingTime, sampleLength, small=True, train=True, encoding='uniform',\n",
    "                 mode='classification'):\n",
    "        self.mode = mode\n",
    "        self.path = datasetPath\n",
    "        if small:\n",
    "            ds = MNIST(datasetPath, train=train, download=True, transform=transforms.Compose([\n",
    "                transforms.ToTensor()]))\n",
    "            self.samples = [ds[i] for i in range(0, 500)]\n",
    "        else:\n",
    "            self.samples = MNIST(datasetPath, train=train, download=True, transform=transforms.Compose([\n",
    "                transforms.ToTensor()]))\n",
    "        self.samplingTime = samplingTime\n",
    "        self.nTimeBins = int(sampleLength / samplingTime)\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, classLabel = self.samples[index]\n",
    "        if self.encoding == 'uniform':\n",
    "            x_spikes = uniform_spike(x, self.nTimeBins)\n",
    "        else:\n",
    "            x_spikes = poisson_spike(x, self.nTimeBins)\n",
    "        if self.mode == 'classification':\n",
    "            desiredClass = torch.zeros((10, 1, 1, 1))\n",
    "            desiredClass[classLabel, ...] = 1\n",
    "            return x_spikes, desiredClass, classLabel\n",
    "        elif self.mode == 'autoencoder':\n",
    "            return x_spikes, x, classLabel\n",
    "        elif self.mode == 'autoencoderSpike':\n",
    "            return x_spikes, x_spikes, classLabel\n",
    "        else:\n",
    "            raise Exception(\n",
    "                'mode is not valid {}. Valid are classification, autoencoder, autoencoderSpike'.format(self.mode))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "# script to transport the weights\n",
    "\n",
    "# tailor_cnn_weights = load_dict(\"tailor_cnn\")\n",
    "# snn_weights = net2.state_dict()\n",
    "\n",
    "# for k in iter(tailor_cnn_weights):\n",
    "#     print(\"Layer {}\".format(k))\n",
    "#     print(tailor_cnn_weights[k].shape)\n",
    "#     if k.startswith('conv') or k.startswith('fc'):\n",
    "#         snn_weights[k] = tailor_cnn_weights[k].reshape(snn_weights[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     1,
     12,
     28,
     139
    ]
   },
   "outputs": [],
   "source": [
    "# some handy function \n",
    "def save_model(name, model):\n",
    "    print(\"Saving models...\")\n",
    "    model.eval()\n",
    "\n",
    "    save_model_filename = 'saved_models/' + name + '.pt'\n",
    "\n",
    "    torch.save(model.state_dict(), save_model_filename)\n",
    "    \n",
    "def load_model(name, model):\n",
    "    model.load_state_dict(torch.load('saved_models/' + name + '.pt'))\n",
    "    \n",
    "def test_acc(dataloader ,model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels =  data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "\n",
    "\n",
    "# the learningStat class    \n",
    "class learningStat():\n",
    "    '''\n",
    "    This class collect the learning statistics over the epoch.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    This class is designed to be used with learningStats instance although it can be used separately.\n",
    "\n",
    "    >>> trainingStat = learningStat()\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.lossSum = 0\n",
    "        self.correctSamples = 0\n",
    "        self.numSamples = 0\n",
    "        self.minloss = None\n",
    "        self.maxAccuracy = None\n",
    "        self.lossLog = []\n",
    "        self.accuracyLog = []\n",
    "        self.bestLoss = False\n",
    "        self.bestAccuracy = False\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset the learning staistics. \n",
    "        This should usually be done before the start of an epoch so that new statistics counts can be accumulated.\n",
    "\n",
    "        Usage:\n",
    "\n",
    "        >>> trainingStat.reset()\n",
    "        '''\n",
    "        self.lossSum = 0\n",
    "        self.correctSamples = 0\n",
    "        self.numSamples = 0\n",
    "\n",
    "    def loss(self):\n",
    "        '''\n",
    "        Returns the average loss calculated from the point the stats was reset.\n",
    "\n",
    "        Usage:\n",
    "\n",
    "        >>> loss = trainingStat.loss()\n",
    "        '''\n",
    "        if self.numSamples > 0: \n",
    "            return self.lossSum/self.numSamples \n",
    "        else:   \n",
    "            return None\n",
    "\n",
    "    def accuracy(self):\n",
    "        '''\n",
    "        Returns the average accuracy calculated from the point the stats was reset.\n",
    "\n",
    "        Usage:\n",
    "\n",
    "        >>> accuracy = trainingStat.accuracy()\n",
    "        '''\n",
    "        if self.numSamples > 0 and self.correctSamples > 0:\n",
    "            return self.correctSamples/self.numSamples  \n",
    "        else:   \n",
    "            return None\n",
    "\n",
    "    def update(self):\n",
    "        '''\n",
    "        Updates the stats of the current session and resets the measures for next session.\n",
    "\n",
    "        Usage:\n",
    "\n",
    "        >>> trainingStat.update()\n",
    "        '''\n",
    "        currentLoss = self.loss()\n",
    "        self.lossLog.append(currentLoss)\n",
    "        if self.minloss is None:\n",
    "            self.minloss = currentLoss\n",
    "        else:\n",
    "            if currentLoss < self.minloss:\n",
    "                self.minloss = currentLoss\n",
    "                self.bestLoss = True\n",
    "            else:\n",
    "                self.bestLoss = False\n",
    "            # self.minloss = self.minloss if self.minloss < currentLoss else currentLoss\n",
    "\n",
    "        currentAccuracy = self.accuracy()\n",
    "        self.accuracyLog.append(currentAccuracy)\n",
    "        if self.maxAccuracy is None:\n",
    "            self.maxAccuracy = currentAccuracy\n",
    "        else:\n",
    "            if currentAccuracy > self.maxAccuracy:\n",
    "                self.maxAccuracy = currentAccuracy\n",
    "                self.bestAccuracy = True\n",
    "            else:\n",
    "                self.bestAccuracy = False\n",
    "            # self.maxAccuracy = self.maxAccuracy if self.maxAccuracy > currentAccuracy else currentAccuracy\n",
    "\n",
    "    def displayString(self):\n",
    "        loss = self.loss()\n",
    "        accuracy = self.accuracy()\n",
    "        minloss = self.minloss\n",
    "        maxAccuracy = self.maxAccuracy\n",
    "\n",
    "        if loss is None:    # no stats available\n",
    "            return None\n",
    "        elif accuracy is None: \n",
    "            if minloss is None: # accuracy and minloss stats is not available\n",
    "                return 'loss = %-12.5g'%(loss)\n",
    "            else:   # accuracy is not available but minloss is available\n",
    "                return 'loss = %-12.5g (min = %-12.5g)'%(loss, minloss)\n",
    "        else:\n",
    "            if minloss is None and maxAccuracy is None: # minloss and maxAccuracy is available\n",
    "                return 'loss = %-12.5g        %-12s   \\taccuracy = %-10.5g        %-10s '%(loss, ' ', accuracy, ' ')\n",
    "            else:   # all stats are available\n",
    "                return 'loss = %-12.5g (min = %-12.5g)  \\taccuracy = %-10.5g (max = %-10.5g)'%(loss, minloss, accuracy, maxAccuracy)\n",
    "\n",
    "class learningStats():\n",
    "    '''\n",
    "    This class provides mechanism to collect learning stats for training and testing, and displaying them efficiently.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    .. code-block:: python\n",
    "    \n",
    "        stats = learningStats()\n",
    "\n",
    "        for epoch in range(100):\n",
    "            tSt = datetime.now()\n",
    "\n",
    "            stats.training.reset()\n",
    "            for i in trainingLoop:\n",
    "                # other main stuffs\n",
    "                stats.training.correctSamples += numberOfCorrectClassification\n",
    "                stats.training.numSamples     += numberOfSamplesProcessed\n",
    "                stats.training.lossSum        += currentLoss\n",
    "                stats.print(epoch, i, (datetime.now() - tSt).total_seconds())\n",
    "            stats.training.update()\n",
    "\n",
    "            stats.testing.reset()\n",
    "            for i in testingLoop\n",
    "                # other main stuffs\n",
    "                stats.testing.correctSamples += numberOfCorrectClassification\n",
    "                stats.testing.numSamples     += numberOfSamplesProcessed\n",
    "                stats.testing.lossSum        += currentLoss\n",
    "                stats.print(epoch, i)\n",
    "            stats.training.update()\n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.linesPrinted = 0\n",
    "        self.training = learningStat()\n",
    "        self.testing  = learningStat()\n",
    "\n",
    "    def update(self):\n",
    "        '''\n",
    "        Updates the stats for training and testing and resets the measures for next session.\n",
    "\n",
    "        Usage:\n",
    "\n",
    "        >>> stats.update()\n",
    "        '''\n",
    "        self.training.update()\n",
    "        self.training.reset()\n",
    "        self.testing.update()\n",
    "        self.testing.reset()\n",
    "\n",
    "    def print(self, epoch, iter=None, timeElapsed=None):\n",
    "        '''\n",
    "        Prints the available learning statistics from the current session on the console.\n",
    "        For Linux systems, prints the data on same terminal space (might not work properly on other systems).\n",
    "\n",
    "        Arguments:\n",
    "            * ``epoch``: epoch counter to display (required).\n",
    "            * ``iter``: iteration counter to display (not required).\n",
    "            * ``timeElapsed``: runtime information (not required).\n",
    "\n",
    "        Usage:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            # prints stats with epoch index provided\n",
    "            stats.print(epoch) \n",
    "\n",
    "            # prints stats with epoch index and iteration index provided\n",
    "            stats.print(epoch, iter=i) \n",
    "            \n",
    "            # prints stats with epoch index, iteration index and time elapsed information provided\n",
    "            stats.print(epoch, iter=i, timeElapsed=time) \n",
    "        '''\n",
    "        print('\\033[%dA'%(self.linesPrinted))\n",
    "        \n",
    "        self.linesPrinted = 1\n",
    "\n",
    "        epochStr   = 'Epoch : %10d'%(epoch)\n",
    "        iterStr    = '' if iter is None else '(i = %7d)'%(iter)\n",
    "        profileStr = '' if timeElapsed is None else ', %12.4f ms elapsed'%(timeElapsed * 1000)\n",
    "\n",
    "        print(epochStr + iterStr + profileStr)\n",
    "        print(self.training.displayString())\n",
    "        self.linesPrinted += 2\n",
    "        if self.testing.displayString() is not None:\n",
    "            print(self.testing.displayString())\n",
    "            self.linesPrinted += 1\n",
    "\n",
    "    def plot(self, figures=(1, 2), saveFig=False, path=''):\n",
    "        '''\n",
    "        Plots the available learning statistics.\n",
    "\n",
    "        Arguments:\n",
    "            * ``figures``: Index of figure ID to plot on. Default is figure(1) for loss plot and figure(2) for accuracy plot.\n",
    "            * ``saveFig``(``bool``): flag to save figure into a file.\n",
    "            * ``path``: path to save the file. Defaule is ``''``.\n",
    "\n",
    "        Usage:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            # plot stats\n",
    "            stats.plot() \n",
    "\n",
    "            # plot stats figures specified\n",
    "            stats.print(figures=(10, 11)) \n",
    "        '''\n",
    "        plt.figure(figures[0])\n",
    "        plt.cla()\n",
    "        if len(self.training.lossLog) > 0:\n",
    "            plt.semilogy(self.training.lossLog, label='Training')\n",
    "        if len(self.testing.lossLog) > 0:\n",
    "            plt.semilogy(self.testing .lossLog, label='Testing')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        if saveFig is True: \n",
    "            plt.savefig(path + 'loss.png')\n",
    "            # plt.close()\n",
    "\n",
    "        plt.figure(figures[1])\n",
    "        plt.cla()\n",
    "        if len(self.training.accuracyLog) > 0:\n",
    "            plt.plot(self.training.accuracyLog, label='Training')\n",
    "        if len(self.testing.accuracyLog) > 0:\n",
    "            plt.plot(self.testing .accuracyLog, label='Testing')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend() \n",
    "        if saveFig is True: \n",
    "            plt.savefig(path + 'accuracy.png')\n",
    "            # plt.close()\n",
    "\n",
    "    def save(self, filename=''):\n",
    "        '''\n",
    "        Saves the learning satatistics logs.\n",
    "\n",
    "        Arguments:\n",
    "            * ``filename``: filename to save the logs. ``accuracy.txt`` and ``loss.txt`` will be appended\n",
    "\n",
    "        Usage:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            # save stats\n",
    "            stats.save() \n",
    "\n",
    "            # save stats filename specified\n",
    "            stats.save(filename='Run101-0.001-') # Run101-0.001-accuracy.txt and Run101-0.001-loss.txt\n",
    "        '''\n",
    "\n",
    "        with open(filename + 'loss.txt', 'wt') as loss:\n",
    "            loss.write('#%11s %11s\\r\\n'%('Train', 'Test'))\n",
    "            for i in range(len(self.training.lossLog)): \n",
    "                loss.write('%12.6g %12.6g \\r\\n'%(self.training.lossLog[i], self.testing.lossLog[i]))\n",
    "\n",
    "        with open(filename + 'accuracy.txt', 'wt') as accuracy:\n",
    "            accuracy.write('#%11s %11s\\r\\n'%('Train', 'Test'))\n",
    "            if self.training.accuracyLog != [None]*len(self.training.accuracyLog):\n",
    "                for i in range(len(self.training.accuracyLog)): \n",
    "                    accuracy.write('%12.6g %12.6g \\r\\n'%(self.training.accuracyLog[i], self.testing.accuracyLog[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\xef'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-81cf40005048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_models/tailor_cnn.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\xef'."
     ]
    }
   ],
   "source": [
    "torch.load('saved_models/tailor_cnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyper params\n",
    "\n",
    "BATCH_SIZE  = 128\n",
    "\n",
    "EPOCH = 30\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(1024)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raw_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Raw_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 12, 5, padding=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(12, 64, 5, padding=1, bias=True)\n",
    "        self.m1 = nn.MaxPool2d(2)\n",
    "        self.m2 = nn.MaxPool2d(2,padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) #(3, 24, 24) -> (64, 20, 20)\n",
    "        x = self.m1(x) #(64, 10, 10)\n",
    "        x = F.relu(self.conv2(x)) #(64, 6, 6)\n",
    "        x = self.m2(x) #(64, 3, 3)\n",
    "        x = x.view(-1, 64 * 6 * 6)    #(64*3*3)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class Tailored_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tailored_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 12, 5, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(12, 64, 5, padding=1, bias=False)\n",
    "        self.a1 = nn.AvgPool2d(2)\n",
    "        self.a2 = nn.AvgPool2d(2,padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) #(3, 24, 24) -> (64, 20, 20)\n",
    "        x = self.a1(x) #(64, 10, 10)\n",
    "        x = F.relu(self.conv2(x)) #(64, 6, 6)\n",
    "        x = self.a2(x) #(64, 3, 3)\n",
    "        x = x.view(-1, 64 * 6 * 6)    #(64*3*3)\n",
    "        x = self.fc1(x) #(10*1*1)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    \n",
    "class Cao_SNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cao_SNN, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cnn = Raw_CNN().to(device)\n",
    "tailored_cnn = Tailored_CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-215966c62aac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtailored_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fc1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "load_model('tailor_cnn', tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\xef'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-81cf40005048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_models/tailor_cnn.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\xef'."
     ]
    }
   ],
   "source": [
    "torch.load('saved_models/tailor_cnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cnn = Raw_CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(raw_cnn.parameters(), lr=0.001)\n",
    "stats = learningStats()\n",
    "\n",
    "## Training\n",
    "\n",
    "# training loop\n",
    "for epoch in tqdm(range(EPOCH)):\n",
    "    # Reset training stats.\n",
    "    stats.training.reset()\n",
    "    tSt = datetime.now()\n",
    "    raw_cnn.train()    \n",
    "    # Training loop.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = raw_cnn(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather training loss stats.\n",
    "        stats.training.lossSum += loss.cpu().data.item()\n",
    "\n",
    "        # Gather the training stats.\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        stats.training.correctSamples += torch.sum(predicted == labels).data.item()\n",
    "        stats.training.numSamples += len(labels)\n",
    "\n",
    "        # Display training stats.\n",
    "        # if i%10 == 0: stats.print(epoch, i, (datetime.now() - tSt).total_seconds())\n",
    "    # Update training stats.\n",
    "    stats.training.update()\n",
    "    # Reset testing stats.\n",
    "    stats.testing.reset()\n",
    "    raw_cnn.eval()    \n",
    "    # Testing loop.\n",
    "    # Same steps as Training loops except loss backpropagation and weight update.\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        outputs = raw_cnn.forward(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        stats.testing.correctSamples += torch.sum(predicted == labels).data.item()\n",
    "        stats.testing.numSamples += len(label)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        stats.testing.lossSum += loss.cpu().data.item()\n",
    "        \n",
    "        # if i%10 == 0: stats.print(epoch, i)\n",
    "\n",
    "\n",
    "    # Update stats.\n",
    "    stats.testing.update()\n",
    "    if epoch%10==0:  stats.print(epoch, timeElapsed=(datetime.now() - tSt).total_seconds())\n",
    "print('Finished Training')\n",
    "save_model('raw_cnn', raw_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc(testloader, raw_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the results.\n",
    "plt.figure(1)\n",
    "plt.semilogy(stats.training.lossLog, label='Training')\n",
    "plt.semilogy(stats.testing.lossLog, label='Testing')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(stats.training.accuracyLog, label='Training')\n",
    "plt.plot(stats.testing.accuracyLog, label='Testing')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
